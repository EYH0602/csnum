\documentclass[a4paper,12pt]{article}
\usepackage{CodeReport}

\begin{document}


\begin{center} % Everything within the center environment is centered.
	{\Large \bf Coding Report 5} % <---- Don't forget to put in the right number
	\vspace{2mm}
\end{center}  


\section{Problem Description}

In this report, we analysis the performance of our implementation of Fixed-point and Newton methods
on the system of nonlinear equations
$$
x_1^2 - 10x_2 + x_2^2 + 8 = 0,\:
x_1x_2^2 + x_1 - 10x_2 + 8 = 0
$$
from HW5.
For each method, we iterate until the residual error is less than $10\mathrm{e}{-6}$.
For the Fixed-point method,
we transform the system of nonlinear equations into the from
$$
x_1 = \frac{x_1^2 + x_2^2 + 8}{10},\:
x_2 = \frac{x_1x_2^2 + x_1 + 8}{10}
$$
as suggested in the homework.

\section{Result}

\begin{table}[h]
\begin{center}
	\begin{tabular}{lrr}
    \toprule
    {} &  Execution Time &  \#Iteration \\
    \midrule
    Fixed Point &        0.063395 &        13.0 \\
    Newton      &        0.040670 &         4.0 \\
    \bottomrule
    \end{tabular}
    \caption{Comparison of Execution Time and Number of Iteration for Each Method}
\end{center}
\end{table}

From the table, we can see that the Newton method is faster than the Fixed-point method
and uses fewer iterations.
This is due to the fact that the Newton method uses the derivative of the function,
which is a special case of the Fixed-point method.
One thing to not is that the difference in execution time 
is not proportion to the difference in number of iterations.
This is because the Newton method uses the autograd functionality of JAX to compute the jacobian matrix
and do a linear system solve for every update.
But in general, since Newton's method uses much less iterations,
it is still faster than the Fixed-point method.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{img/report5_cell_8_output_1.png}
    \caption{Residual Error v.s. Iteration for Each Method}
    \label{fig:0}   
\end{figure}

The plot also conforms that the Newton's method converges using fewer iterations than the Fixed-point method.
It also shows that for Newton's method, each update is more aggressive than the Fixed-point method.

\section{Academic Integrity}
On my personal integrity as a student and member of the UCD community, I have not given nor received any unauthorized assistance on this assignment.


\section{Appendix}

\begin{lstlisting}[language=Python, title=\url{numerical_methods/root_finding_1_var.py}]
import jax.numpy as jnp
from jax import grad
from typing import Tuple, Callable, Dict, List, TypeVar
from logging import warning


Calcable = TypeVar("Calcable")


def general_iter_method(
    succ: Callable[[List[Calcable]], Calcable],
    p0: Calcable,
    tol: float,
    max_iter: int,
    converged: Callable[[Calcable, Calcable, float], bool],
    method: str = "",
    return_all=True,
):
    """General Iterative Method (gim)

    Args:
        succ (Callable[List[float], float]):
            compute the new approximation p from previous approximations
        p0 (float): initial approximation
        tol (float): tolerance
        max_iter (int):
            maximum number of iterations;
            if converged before this, pad with the root it finds
        method (str, optional): method name that is calling gim

    Returns:
        List[float]: the sequence of approximations used by the method
    """
    p: List[Calcable] = []
    p.append(p0)
    for i in range(1, max_iter):
        p.append(succ(p))
        if converged(p[i], p[i - 1], tol):
            p += [p[i]] * (max_iter - i - 1)
            return p if return_all else p[-1]

    warning(f"{method} fail to converge in {max_iter} iterations")
    return p if return_all else p[-1]
\end{lstlisting}
\lstinputlisting[language=Python, title=\url{numerical_methods/nonlinear_iter.py}]{../numerical_methods/nonlinear_system.py}
\lstinputlisting[language=Python, title=main.py]{report5.py}

\end{document}